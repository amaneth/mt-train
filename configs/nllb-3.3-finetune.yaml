model:
  name: "facebook/nllb-200-distilled-600M"
  use_cache: false
  device_map: "auto"


datasets:
  train:
    name: "amaniopia/merged_train"
    split: "train"
    columns:
      source: "source"
      target: "target"
      src_lang: "src_lang"
      tgt_lang: "tgt_lang"

  validation:
    name: "amaniopia/flores-merged"
    split: "train"
    columns:
      source: "source"
      target: "target"
      src_lang: "src_lang"
      tgt_lang: "tgt_lang"


tokenization:
  max_length: 512
  padding: "max_length"
  truncation: true


training:
  num_train_epochs: 1
  learning_rate: 5e-5
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 4
  eval_accumulation_steps: 4
  fp16: true
  fp16_full_eval: true
  lr_scheduler_type: "constant"
  eval_strategy: "steps"      # options: "steps", "epoch"
  eval_steps: 10
  save_strategy: "steps"
  save_steps: 10
  save_total_limit: 2
  logging_steps: 10
  report_to: "tensorboard"           # options: "tensorboard", "wandb", "none"
  push_to_hub: true
  private_repo: true
  strategy: "every_save"
output:
  hf_username: "amaniopia"
  dir: "amaniopia/nllb-200-3.3B-finetuned"
